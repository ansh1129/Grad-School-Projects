{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Logreg Text Model\n",
    "\n",
    "<br>\n",
    "**Author** Mary Letey <br>\n",
    "**Date** 4/30/18 <br>\n",
    "**File** basiclogreg.ipynb <br>\n",
    "**Purpose** Using a basic logistic regression model to analyze the affects of text data on stock increases/decreases\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import matplotlib.pylab as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_csv('../data/TOTAL-dataset.csv')\n",
    "\n",
    "date = alldata[\"date\"]\n",
    "\n",
    "hp_x = alldata[\"hp-body\"]\n",
    "ibm_x = alldata[\"ibm-body\"]\n",
    "sea_x = alldata[\"seagate-body\"]\n",
    "west_x = alldata[\"western-digital-body\"]\n",
    "\n",
    "hp_price = alldata[\"hp_Last Price\"]\n",
    "ibm_price = alldata[\"ibm_Last Price\"]\n",
    "sea_price = alldata[\"seagate_Last Price\"]\n",
    "west_price = alldata[\"westdig_Last Price\"]\n",
    "\n",
    "# print(ibm_price)\n",
    "# ibm_price[4]\n",
    "\n",
    "def makeBinary(stock):\n",
    "    biny = []\n",
    "    biny.append(1)\n",
    "    for i in range(1,len(stock)):\n",
    "        if stock[i] >= stock[i-1]:\n",
    "            biny.append(1)\n",
    "        else:\n",
    "            biny.append(0)\n",
    "    return biny\n",
    "\n",
    "hp_y = makeBinary(hp_price)\n",
    "ibm_y = makeBinary(ibm_price)\n",
    "sea_y = makeBinary(sea_price)\n",
    "west_y = makeBinary(west_price)\n",
    "\n",
    "def looseNAN(df):\n",
    "    ind = [i for i, x in enumerate(df[\"x\"]) if pd.isnull(x)]\n",
    "    df = df.drop(ind)\n",
    "    return df\n",
    "    \n",
    "hp_df = pd.DataFrame({\"date\": date, \"x\": hp_x, \"y\": hp_y})\n",
    "hp_df = looseNAN(hp_df)\n",
    "hp_x = list(hp_df[\"x\"])\n",
    "hp_y = list(hp_df[\"y\"])\n",
    "\n",
    "ibm_df = pd.DataFrame({\"date\": date, \"x\": ibm_x, \"y\": ibm_y})\n",
    "ibm_df = looseNAN(ibm_df)\n",
    "ibm_x = ibm_df[\"x\"]\n",
    "ibm_y = ibm_df[\"y\"]\n",
    "\n",
    "sea_df = pd.DataFrame({\"date\": date, \"x\": sea_x, \"y\": sea_y})\n",
    "sea_df = looseNAN(sea_df)\n",
    "sea_x = sea_df[\"x\"]\n",
    "sea_y = sea_df[\"y\"]\n",
    "\n",
    "west_df = pd.DataFrame({\"date\": date, \"x\": west_x, \"y\": west_y})\n",
    "west_df = looseNAN(west_df)\n",
    "west_x = west_df[\"x\"]\n",
    "west_y = west_df[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logreg Class Definitions\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        \n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        self.vectorizer = CountVectorizer()\n",
    "\n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.transform(examples)\n",
    "\n",
    "    def show_top10(self):\n",
    "        \"\"\"\n",
    "        prints the top 10 features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "                \n",
    "    def train_model(self, x, y, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        \n",
    "        # get training features and labels \n",
    "        self.X_train = self.build_train_features(x)\n",
    "        self.y_train = np.array(y, dtype=int)\n",
    "        \n",
    "        # train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def model_predict(self, xtest):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \"\"\"\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features(xtest)\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "222 30\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the FeatEngr clas \n",
    "feat = FeatEngr()\n",
    "\n",
    "def getElementsWithIndex(index, source):\n",
    "    result = []\n",
    "    for i, ind in enumerate(index):\n",
    "        val = source[ind]\n",
    "        result.append(val)\n",
    "    return result\n",
    "        \n",
    "# split for testing\n",
    "#X_train, X_test, y_train, y_test = sklearn.model_selection.StratifiedKFold(n_splits=2, shuffle=True)\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "for train_index, test_index in skf.split(hp_x, hp_y):\n",
    "    X_train = getElementsWithIndex(train_index, hp_x)\n",
    "    X_test = getElementsWithIndex(test_index, hp_x)\n",
    "    y_train = getElementsWithIndex(train_index, hp_y)\n",
    "    y_test = getElementsWithIndex(test_index, hp_y)\n",
    "\n",
    "print(y_test)\n",
    "ind0 = [i for i,y in enumerate(y_test) if y==0]\n",
    "ind1 = [i for i,y in enumerate(y_test) if y==1]\n",
    "sum0 = len(ind0)\n",
    "sum1 = len(ind1)\n",
    "print(sum0,sum1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-0821e882a70a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msum0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msum1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "y = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "x = [5,6,7,8,9,9,10,9]\n",
    "d = [9,9,9,9,9,9,9,9]\n",
    "sums = sum(x == d)\n",
    "sum0 = sum(y_test == 0)\n",
    "sum1 = sum(y_test == 1)\n",
    "print(sum0,sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_test_split(hp_x, hp_y, test_size=0.4, random_state=6)\n",
    "\n",
    "# Train your Logistic Regression classifier \n",
    "feat.train_model(X_train, y_train, random_state=1230)\n",
    "\n",
    "# Make predictions\n",
    "pred = feat.model_predict(X_test)\n",
    "y_test = array(y_test)\n",
    "\n",
    "sums = sum(pred == y_test)\n",
    "trainacc = sums/len(y_test)\n",
    "print(\"Testing acuracy is\",100*trainacc)\n",
    "\n",
    "sum1 = 0\n",
    "for i in range(0, len(pred)):\n",
    "    bool = (pred[i] == 1) and (y_test[i] == 1)\n",
    "    if bool:\n",
    "        sum1 += 1\n",
    "den1 = sum(y_test == 1)\n",
    "print(\"Model predicts\",100*sum1/den1,\"percent of stock increases\")\n",
    "\n",
    "sum0 = 0\n",
    "for i in range(0, len(pred)):\n",
    "    bool = (pred[i] == 0) and (y_test[i] == 0)\n",
    "    if bool:\n",
    "        sum0 += 1\n",
    "den0 = sum(y_test == 0)\n",
    "print(\"Model predicts\",100*sum0/den0,\"percent of stock decreases\")\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(25,12))\n",
    "\n",
    "ax[0].scatter(range(0,len(y_test)),y_test,label=\"Actual Stock Changes\")\n",
    "ax[0].legend();\n",
    "ax[1].scatter(range(0,len(pred)),pred,label=\"Predicted Stock Changes\")\n",
    "ax[1].legend();\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
